# -*- coding: utf-8 -*-
"""LDPTrace -Implementation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tcbsYUuTotAeci29e7mcaxIy2n-MvA2b
"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /

# Commented out IPython magic to ensure Python compatibility.
# Remove existing directory, if present already!
# %rm -rf /LDPTrace-implementation/

!git clone https://github.com/sandxxax/LDPTrace-implementation.git

# Commented out IPython magic to ensure Python compatibility.
# Datasets are stored in this directory
# %cd /LDPTrace-implementation/LDPTrace/data/
# %ls
# Below displayed are the datasets, Lets display the first 20 lines of dataset, and see what it stores

# Open the .dat file in binary mode
file_path = 'original_oldenburg.dat'

with open(file_path, 'rb') as file:
    # Read the first 10 lines (adjust the number as needed)
    for i in range(20):
        line = file.readline()
        print(line.decode('utf-8'))  # Assuming it's a text file, decode the binary data to a string

# Commented out IPython magic to ensure Python compatibility.
# %cd /LDPTrace-implementation/LDPTrace/code/
# Go to the /code directory to run transformdataset.py file to preprocess that dataset.
# %run transformdataset.py
# display the contents of the /data folder using ls command below
# %cd /LDPTrace-implementation/LDPTrace/data/
# %ls
# We can see that, oldenburg.dat is created here.!

# Lets read the contents of the processed oldenburg.dat file created just now..

# Open the .dat file in binary mode
file_path = 'oldenburg.dat'

with open(file_path, 'rb') as file:
    # Read the first 10 lines (adjust the number as needed)
    for i in range(20):
        line = file.readline()
        print(line.decode('utf-8'))  # Assuming it's a text file, decode the binary data to a string

# We can see that, oldenburg.dat is in the proper format required for the main function to run!
# Now lets go to /code/ directory, and run the main.py function

# Commented out IPython magic to ensure Python compatibility.
# %cd /LDPTrace-implementation/LDPTrace/code/

# We need to give the --dataset as an argument along with its value, "oldenburg"
# We also need to give --re_syn argument, because this is the first time, we are running this main function, and we dont have the '.json' file
# '.json' file is the attribute describer, which details the structure and categories of the oldenburg.dat
# '.json' file is needed to create the synthetic dataset!

# Since there is no '.json' file, lets create the file by running the main.py file with --re_syn argument

# %run main.py --dataset oldenburg --re_syn

# Commented out IPython magic to ensure Python compatibility.
# Now, lets see if the '.json' file is created or not in the /data folder ?
# %cd /LDPTrace-implementation/LDPTrace/data/
# %ls

# Commented out IPython magic to ensure Python compatibility.
# Now, lets check if the synthetic dataset or 'pickle file' is generated or not in the /data/oldenburg directory ?
# %cd /LDPTrace-implementation/LDPTrace/data/oldenburg/
# %ls

# Lets display the contents or first few lines of the '.pkl' file in the above cell
import pickle

file_path = 'syn_oldenburg_eps_1.0_max_0.9_grid_6.pkl'

# Open the pickle file in binary mode
with open(file_path, 'rb') as file:
    data = pickle.load(file)

# Display the top 50 lines or elements
for i, item in enumerate(data[:50]):
    print(f"Line {i+1}: {item}")

#===================================THE END=====================================